{
  "data": [
    {
      "architecture": {
        "instruct_type": null,
        "modality": "text+image->text",
        "tokenizer": "GPT"
      },
      "context_length": 128000,
      "created": 1721260800,
      "description": "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
      "id": "openai/gpt-4o-mini",
      "name": "OpenAI: GPT-4o-mini",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.0000006",
        "image": "0.007225",
        "prompt": "0.00000015",
        "request": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "is_moderated": true,
        "max_completion_tokens": 16384
      }
    },
    {
      "architecture": {
        "instruct_type": null,
        "modality": "text+image->text",
        "tokenizer": "GPT"
      },
      "context_length": 128000,
      "created": 1715558400,
      "description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
      "id": "openai/gpt-4o",
      "name": "OpenAI: GPT-4o",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00001",
        "image": "0.003613",
        "prompt": "0.0000025",
        "request": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "is_moderated": true,
        "max_completion_tokens": 16384
      }
    },
    {
      "architecture": {
        "instruct_type": "llama3",
        "modality": "text->text",
        "tokenizer": "Llama3"
      },
      "context_length": 131072,
      "created": 1733506137,
      "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
      "id": "meta-llama/llama-3.3-70b",
      "name": "Meta: Llama 3.3 70B",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.0000004",
        "image": "0",
        "prompt": "0.00000013",
        "request": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    },
    {
      "architecture": {
        "instruct_type": "deepseek-r1",
        "modality": "text->text",
        "tokenizer": "Llama3"
      },
      "context_length": 131072,
      "created": 1737663169,
      "description": "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "id": "deepseek/deepseek-r1-distill-llama-70b",
      "name": "DeepSeek: R1 Distill Llama 70B",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00000069",
        "image": "0",
        "input_cache_read": "0",
        "input_cache_write": "0",
        "internal_reasoning": "0",
        "prompt": "0.00000023",
        "request": "0",
        "web_search": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "is_moderated": false,
        "max_completion_tokens": 8192
      }
    }
  ]
}